{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/anaconda2/lib/python2.7/site-packages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_VECTOR_PATH = 'GoogleNews-vectors-negative300.bin'\n",
    "DUMPED_VECTOR_DIR = '/vectors_mb_new/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary_size(data):\n",
    "    words = set()\n",
    "\n",
    "    for doc in data:\n",
    "        tokens = doc.split()\n",
    "        for t in tokens:\n",
    "            words.add(t)\n",
    "\n",
    "    print('\\n'.join(sorted(words)))\n",
    "    print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = codecs.open(gloveFile, 'r', encoding='latin-1').read().split('\\n')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        # print(word)\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\", len(model), \" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_into_sequences(df, colname):\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    #if(df[colname].values) is str:\n",
    "    tokenizer.fit_on_texts(str(df[colname].values))\n",
    "    sequences = tokenizer.texts_to_sequences(str(df[colname].values))\n",
    "    \n",
    "    sl = max(list(map(lambda x: len(x), sequences)))\n",
    "    print(sl)\n",
    "    # print(df[colname].values[:10])\n",
    "    print(sequences[:33])\n",
    "    print(max(tokenizer.word_index.values()), len(tokenizer.word_index.values()))\n",
    "\n",
    "    lens = list(map(lambda s: len(s), sequences))\n",
    "    print(len(lens), max(lens))\n",
    "    data = pad_sequences(sequences, maxlen=sl)\n",
    "    print(data[:10])\n",
    "\n",
    "  \n",
    "    index_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "  \n",
    "    # Load word embeddings and create embedding matrix\n",
    "    print('Loading Embeddings')\n",
    "    model = word2vec.KeyedVectors.load_word2vec_format(WORD_EMBEDDING_VECTOR_PATH, binary=True, encoding='utf-8')\n",
    "    # model = loadGloveModel(config.WORD_EMBEDDING_VECTOR_PATH)\n",
    "\n",
    "    print('Loaded')\n",
    "    print(model['try'])\n",
    "    #\n",
    "    nf = set()\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "    counter = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "\n",
    "        embedding_vector = np.zeros(300)\n",
    "        if word in model:\n",
    "            embedding_matrix[i] = model[word]\n",
    "            counter += 1\n",
    "        else:\n",
    "            nf.add(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    print(len(nf))\n",
    "    print(len(data))\n",
    "    print(len(embedding_matrix))\n",
    "    joblib.dump(embedding_matrix,'mb_voc_embeddings.pkl')\n",
    "    joblib.dump(data,'mb_sequences.pkl')\n",
    "\n",
    "    print(len(data))\n",
    "    print(len(embedding_matrix))\n",
    "    print(data.shape)\n",
    "    print(embedding_matrix.shape)\n",
    "    print(counter)\n",
    "\n",
    "    print(nf)\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('mb_train_trial_test_new_raw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from sklearn.externals import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/Keras-2.0.8-py3.5.egg/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[], [1], [4], [1], [], [1], [4], [1], [], [1], [4], [1], [], [], [], [], [], [], [], [], [], [9], [14], [10], [1], [], [8], [5], [8], [2], [11], [6], [5]]\n",
      "28 28\n",
      "206 1\n",
      "[[0]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Loading Embeddings\n",
      "Loaded\n",
      "[  2.40234375e-01   2.01171875e-01   1.62109375e-01   2.08984375e-01\n",
      "   1.15966797e-02  -2.58789062e-02   2.33398438e-01  -3.56445312e-02\n",
      "  -1.05957031e-01  -1.47460938e-01  -1.02539062e-02   1.03149414e-02\n",
      "  -6.17675781e-02  -2.79541016e-02  -2.08007812e-01  -4.12597656e-02\n",
      "   7.81250000e-02   2.53906250e-01   2.32421875e-01  -1.50390625e-01\n",
      "   4.78515625e-02   7.66601562e-02   2.22656250e-01  -1.48437500e-01\n",
      "  -1.20849609e-02  -1.40625000e-01  -4.07714844e-02   7.27539062e-02\n",
      "   4.57763672e-03   3.03955078e-02   1.44531250e-01   1.35742188e-01\n",
      "  -3.32031250e-02  -1.74804688e-01  -6.64062500e-02   1.56250000e-01\n",
      "   1.55273438e-01   8.05664062e-03   1.12792969e-01   8.54492188e-03\n",
      "   1.59179688e-01  -5.12695312e-02   1.66992188e-01  -8.25195312e-02\n",
      "  -1.87500000e-01   2.81250000e-01  -5.00488281e-02   8.05664062e-02\n",
      "   1.92260742e-03   1.01074219e-01  -7.32421875e-02   1.62109375e-01\n",
      "   5.32226562e-02  -5.34057617e-03   2.84423828e-02  -3.36914062e-02\n",
      "  -7.26318359e-03  -1.37695312e-01   1.05957031e-01  -1.49536133e-02\n",
      "   2.24609375e-02  -6.17675781e-02  -2.18750000e-01  -8.93554688e-02\n",
      "   1.15234375e-01  -2.35595703e-02   9.58251953e-03   2.85156250e-01\n",
      "   1.39770508e-02   7.08007812e-02   4.71191406e-02   1.14746094e-01\n",
      "   3.46679688e-02   4.37011719e-02  -3.51562500e-01  -2.50000000e-01\n",
      "   6.49414062e-02   8.64257812e-02  -7.75146484e-03  -5.46875000e-02\n",
      "  -1.54296875e-01  -7.95898438e-02   6.25000000e-02   4.12597656e-02\n",
      "   2.29492188e-02  -1.06445312e-01  -1.08398438e-01   9.61914062e-02\n",
      "  -2.19726562e-01   1.66992188e-01   1.53320312e-01   7.71484375e-02\n",
      "  -1.79687500e-01  -2.55859375e-01   8.39843750e-02  -2.32421875e-01\n",
      "   7.22656250e-02   5.93261719e-02  -2.03857422e-02   9.96093750e-02\n",
      "  -4.76837158e-05  -9.64355469e-03  -1.14746094e-02   2.74658203e-03\n",
      "   3.01513672e-02  -3.22265625e-02  -1.19140625e-01  -4.51660156e-02\n",
      "   7.71484375e-02   1.32812500e-01  -2.36328125e-01  -1.61132812e-02\n",
      "  -1.13281250e-01  -5.63964844e-02   2.85644531e-02  -2.66113281e-02\n",
      "   2.55126953e-02  -1.64794922e-02  -6.19506836e-03   2.47802734e-02\n",
      "   1.51367188e-02   8.88671875e-02  -1.00097656e-01   7.37304688e-02\n",
      "   6.25000000e-02  -1.66992188e-01  -1.22558594e-01  -2.28515625e-01\n",
      "  -4.34570312e-02  -4.85839844e-02  -4.51660156e-02  -5.56640625e-02\n",
      "  -1.15356445e-02   6.34765625e-02  -4.68750000e-02  -1.33789062e-01\n",
      "   1.83593750e-01   9.70458984e-03   7.12890625e-02   1.92382812e-01\n",
      "   1.12792969e-01  -2.00195312e-01  -3.39355469e-02  -3.24707031e-02\n",
      "   6.83593750e-02   2.25585938e-01  -1.75781250e-01  -5.15136719e-02\n",
      "   4.00390625e-02  -9.37500000e-02  -7.32421875e-02   6.88476562e-02\n",
      "  -1.13281250e-01   2.16796875e-01   7.76367188e-02  -1.68457031e-02\n",
      "   7.22656250e-02  -1.63085938e-01  -1.25976562e-01   1.82617188e-01\n",
      "  -1.08032227e-02   1.62109375e-01   5.49316406e-04  -1.81640625e-01\n",
      "   3.28063965e-03  -2.18750000e-01   2.12890625e-01  -1.31835938e-01\n",
      "   1.60156250e-01  -1.11328125e-01  -1.13769531e-01   9.61914062e-02\n",
      "  -1.97265625e-01   1.35742188e-01   1.31835938e-01  -6.68945312e-02\n",
      "   2.22656250e-01   1.16699219e-01  -1.49414062e-01   1.74804688e-01\n",
      "  -1.10839844e-01  -1.20117188e-01   7.81250000e-02   4.71191406e-02\n",
      "   3.78417969e-03  -2.32421875e-01  -1.01562500e-01  -3.27148438e-02\n",
      "   2.24609375e-01  -1.00097656e-01   1.89208984e-03   1.03515625e-01\n",
      "   2.57812500e-01  -6.89697266e-03   1.73828125e-01  -8.30078125e-02\n",
      "   1.85546875e-01   1.19140625e-01  -2.85156250e-01  -2.59765625e-01\n",
      "   8.98437500e-02   2.37304688e-01  -1.30859375e-01   9.37500000e-02\n",
      "   1.11328125e-01   6.64062500e-02  -2.44140625e-02   4.54101562e-02\n",
      "   8.83789062e-02  -6.93359375e-02  -1.51367188e-02   1.56250000e-01\n",
      "  -7.71484375e-02   2.36328125e-01  -7.76367188e-02  -1.06201172e-02\n",
      "   5.02929688e-02   7.47680664e-03   1.50756836e-02  -7.56835938e-02\n",
      "   1.18408203e-02  -1.01562500e-01  -1.58203125e-01  -4.51660156e-02\n",
      "  -3.46679688e-02   1.11328125e-01   1.32812500e-01   6.34765625e-02\n",
      "   5.07812500e-02   4.07714844e-02   8.20312500e-02  -2.08984375e-01\n",
      "  -1.02539062e-02  -2.06947327e-04   1.20605469e-01  -6.49414062e-02\n",
      "  -1.09863281e-01  -1.57226562e-01  -5.24902344e-02   1.07421875e-01\n",
      "  -8.60595703e-03   1.87500000e-01   8.49609375e-02  -1.18652344e-01\n",
      "  -2.56347656e-02   7.22656250e-02  -4.95605469e-02   2.47070312e-01\n",
      "   1.52343750e-01  -1.33666992e-02  -1.88476562e-01   7.81250000e-02\n",
      "   1.58203125e-01   6.00585938e-02   1.91406250e-01   8.10546875e-02\n",
      "   4.76074219e-02  -7.27539062e-02  -1.09252930e-02  -2.86865234e-02\n",
      "  -3.18359375e-01   3.12500000e-01  -5.37109375e-02   8.25195312e-02\n",
      "  -1.59912109e-02   7.11059570e-03   5.78613281e-02   5.68847656e-02\n",
      "  -2.05078125e-01   8.25195312e-02   1.56250000e-01   2.34375000e-01\n",
      "   8.78906250e-02   2.17773438e-01   5.76171875e-02   1.55639648e-02\n",
      "  -6.29882812e-02  -4.06250000e-01  -8.83789062e-02   6.29882812e-02\n",
      "  -1.47460938e-01  -8.39843750e-02   9.86328125e-02   8.64257812e-02\n",
      "  -2.81982422e-02   1.26342773e-02  -1.62109375e-01   8.98437500e-02\n",
      "  -1.19628906e-01  -1.13769531e-01   1.12792969e-01   1.76757812e-01\n",
      "  -1.08398438e-01  -4.39453125e-03  -1.48437500e-01  -6.49414062e-02\n",
      "   1.95312500e-01   3.49121094e-02   3.14941406e-02   1.45263672e-02]\n",
      "2\n",
      "206\n",
      "29\n",
      "206\n",
      "29\n",
      "(206, 1)\n",
      "(29, 300)\n",
      "26\n",
      "{\"'\", 'a'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_into_sequences(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pack_data_to_format():\n",
    "    predictions = joblib.load('dl_predictions.pkl')\n",
    "    \n",
    "    test_json = json.load(open('Microblogs_Testdata.json', 'r'))\n",
    "    # print(test_json[:5])\n",
    "    n=len(test_json)\n",
    "    pred_list = []\n",
    "    for i in range(n):\n",
    "        data[i] = None\n",
    "\n",
    "    for i in range(len(test_json)):\n",
    "        data = {'id': test_json[i]['id'], 'cashtag': test_json[i]['cashtag'], 'sentiment score': str(predictions[i][0])}\n",
    "        pred_list.append(data)\n",
    "  \n",
    "\n",
    "    json.dump(pred_list, open('dl_submission2.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(mode):\n",
    "    loaded_feature_list = []\n",
    "\n",
    "    for feature_name in config.features_to_use_mb:\n",
    "        if mode != 'mb' and (feature_name == 'cashtag' or feature_name == 'source'):\n",
    "            continue\n",
    "        print('\\n---------------------------------------------')\n",
    "        print( 'Loading {} from '.format(feature_name), end=' ' )\n",
    "\n",
    "        # Microblog\n",
    "        if mode == 'mb':\n",
    "            filename =  'mb_' + feature_name + '.pkl'\n",
    "        else:\n",
    "            filename = 'hl_' + feature_name + '.pkl'\n",
    "        print(filename)\n",
    "\n",
    "        loaded_feature = joblib.load( filename )\n",
    "\n",
    "        if not isinstance(loaded_feature, np.ndarray):\n",
    "            loaded_feature = loaded_feature.toarray()\n",
    "        print('Shape =  {}, type = {}'.format(loaded_feature.shape, type(loaded_feature)))\n",
    "        loaded_feature_list.append( loaded_feature )\n",
    "        print('---------------------------------------------')\n",
    "\n",
    "    return combine_features(loaded_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pack_data_to_format()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #----------------------------------------------------------------\n",
    "    # Headline\n",
    "    MOOD = 'mb'\n",
    "\n",
    "    print('Loading X')\n",
    "    X = joblib.load(config.DUMPED_VECTOR_DIR+'mb_sequences.pkl')\n",
    "    features = get_features(MOOD)\n",
    "    print(X.shape, features.shape)\n",
    "    print('Loading Y')\n",
    "\n",
    "    if MOOD == 'hl':\n",
    "        y = joblib.load( 'headline_scores.pkl' )\n",
    "    else:\n",
    "        y = joblib.load('mb_scores.pkl')\n",
    "    print(type(X), type(y))\n",
    "    print(len(y))\n",
    "\n",
    "    # Microblog Split\n",
    "    X_train, X_dev, X_test, Y_train, Y_dev = X[:1693], X, X[1693:], y[:1693], y\n",
    "    features_train, features_dev, features_test = features[:1693], features, features[1693:]\n",
    "    # ----------------------------------------------------------------\n",
    "    print(len(X_train), len(X_test), len(X_dev), len(Y_train), len(Y_dev))\n",
    "    print(len(y))\n",
    "    deep_learning_cv(X_train, Y_train, 10, X_dev, Y_dev, X_test, features_train, features_dev, features_test)\n",
    "    # final_predict(X_train, Y_train, X_test, features_train, features_dev, features_test)\n",
    "    print('MICROBLOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_learning_cv(X_train, y_train, k, X_dev, y_dev, X_test, company_train, company_dev, company_test):\n",
    "    global X_DIM, Y_DIM\n",
    "    X_DIM = X_train[0].shape[0]\n",
    "    y_train = np.array(y_train)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    Y_DIM = 1\n",
    "    y_test = np.array(y_dev).reshape(1,-1)\n",
    "\n",
    "    kfold_cv = KFold(n_splits=k, shuffle=True, random_state=7)\n",
    "    fold_counter = 1\n",
    "    cos_train = []\n",
    "    cos_test = []\n",
    "    cos_trial = []\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # SVR\n",
    "    # -------------------------------------------------\n",
    "    # param_grid = {'C': [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.00001, 0.000001, 0.0000001]}\n",
    "    #\n",
    "    # regressor = LinearSVR()\n",
    "    # best_regressor = GridSearchCV(regressor, param_grid, cv=k, verbose=0, n_jobs=-1,\n",
    "    #                               scoring=make_scorer(cosine_similarity))\n",
    "    # best_regressor.fit(company_train, y_train)\n",
    "    # print(\"best parameter: \", best_regressor.best_params_)\n",
    "    # print(\"best score: \", best_regressor.best_score_)\n",
    "    # print(best_regressor.grid_scores_)\n",
    "\n",
    "\n",
    "    # print('LSTM, Epoch', NB_EPOCH)\n",
    "    print('Fold\\t Training\\t \\t Test \\t\\t\\t\\t Trial')\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    for train_index, test_index in kfold_cv.split(X_train):\n",
    "        print('Size', len(train_index), len(test_index))\n",
    "        # print('Running Fold {}'.format(fold_counter))\n",
    "        # print('------------------------------------------')\n",
    "        print('Fold {}'.format(fold_counter), end='\\t')\n",
    "\n",
    "        X_tr, X_ts = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_ts = y_train[train_index], y_train[test_index]\n",
    "        C_tr, C_ts = company_train[train_index], company_train[test_index]\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, verbose=0),\n",
    "        ]\n",
    "        # regressor = KerasRegressor(build_fn=attention_imp_merge_exp, nb_epoch=NB_EPOCH, batch_size=BATCH_SIZE, verbose=1)\n",
    "        regressor = linear_model.ElasticNet(random_state=7)\n",
    "        # regressor = LinearSVR(C=0.1)\n",
    "        # print('Training Model')\n",
    "        if MERGE == True:\n",
    "            # regressor.fit([X_tr, C_tr], y_tr, callbacks=callbacks)\n",
    "            regressor.fit([X_tr, C_tr, X_tr], y_tr)\n",
    "        else:\n",
    "            print('NOT MERGE', C_tr.shape, y_tr.shape)\n",
    "            regressor.fit(C_tr, y_tr)\n",
    "            # regressor.fit(X_tr, y_tr)\n",
    "\n",
    "        # print('Predicting Tests')\n",
    "        if MERGE == True:\n",
    "            # predictions = regressor.predict([X_ts, C_ts])\n",
    "            # predictions = np.array(predictions).reshape(1, -1)\n",
    "            # train_predictions = regressor.predict([X_tr, C_tr])\n",
    "            # trial_predictions = regressor.predict([X_dev, company_dev])\n",
    "            predictions = regressor.predict([X_ts, C_ts, X_ts])\n",
    "            predictions = np.array(predictions).reshape(1,-1)\n",
    "            train_predictions = regressor.predict([X_tr, C_tr, X_tr])\n",
    "            trial_predictions = regressor.predict([X_dev, company_dev, X_dev])\n",
    "        else:\n",
    "            predictions = regressor.predict(C_ts)\n",
    "            # predictions = regressor.predict(X_ts)\n",
    "            # test_predictions = regressor.predict(X_dev)\n",
    "            predictions = np.array(predictions).reshape(1, -1)\n",
    "            train_predictions = regressor.predict(C_tr)\n",
    "            # trial_predictions = regressor.predict(company_test)\n",
    "\n",
    "        cos_train.append( cosine_similarity(y_tr, train_predictions)[0][0] )\n",
    "        cos_test.append(cosine_similarity(y_ts, predictions)[0][0])\n",
    "        # cos_trial.append(cosine_similarity(y_test, trial_predictions)[0][0])\n",
    "\n",
    "        print('{}\\t{}'.format(cos_train[-1], cos_test[-1]))\n",
    "        # print('{}\\t{}\\t{}'.format(cos_train[-1], cos_test[-1], cos_trial[-1]))\n",
    "        # print(\"Cosine Similarity for Training fold \", cos_train[-1])\n",
    "        # print(\"Cosine Similarity for Test fold \", cos_test[-1])\n",
    "        # print(\"Cosine Similarity for Trial Data \", cos_trial[-1])\n",
    "\n",
    "        fold_counter += 1\n",
    "\n",
    "    print('---------------')\n",
    "    print('Final Result')\n",
    "    print('Training fold cosine mean : ', np.mean(cos_train))\n",
    "    print('Testing fold cosine mean : ', np.mean(cos_test))\n",
    "    print('Trial fold cosine mean : ', np.mean(cos_trial))\n",
    "    print('{},{},{}'.format(np.mean(cos_train), np.mean(cos_test), np.mean(cos_trial)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
