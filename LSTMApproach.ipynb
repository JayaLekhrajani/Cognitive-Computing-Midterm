{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/lib/python2.7/dist-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "import unitok3.unitok.configs.english\n",
    "import unitok3.unitok as tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data='Headline_Trainingdata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data='Headlines_Testdata_withscores.json'\n",
    "trail_data='Headline_Trialdata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/anaconda2/lib/python2.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unitok_tokens(text):\n",
    "    '''Tokenises using unitok http://corpus.tools/wiki/Unitok the text. Given\n",
    "    a string of text returns a list of strings (tokens) that are sub strings\n",
    "    of the original text. It does not return any whitespace.\n",
    "    String -> List of Strings\n",
    "    '''\n",
    "\n",
    "    tokens = tokenize(text, unitok3.unitok.configs.english)\n",
    "    return [token for tag, token in tokens if token.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_length(texts):\n",
    "    '''Given a list of strings it will return the length of the string with the\n",
    "    most tokens. Where length is measured in number of tokens. unitok_tokens\n",
    "    method is used to identify tokens.\n",
    "    List of strings -> Integer\n",
    "    '''\n",
    "\n",
    "    max_token_length = 0\n",
    "    for text in texts:\n",
    "        tokens = unitok_tokens(text)\n",
    "        if len(tokens) > max_token_length:\n",
    "            max_token_length = len(tokens)\n",
    "    return max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(texts, wordvec_model, max_token_length):\n",
    "    '''Given a list of Strings a word2vec model and the maximum token length\n",
    "    it will return a 3 dimensional numpy array of the following shape:\n",
    "    (number of texts, word2vec model vector size, max token length).\n",
    "    Each text will have each token mapped to a vector in the word2vec model. If\n",
    "    the token does not exist then a vector of zeros will be inserted instead.\n",
    "    The vector of zero applices when the text has no more tokens but has not\n",
    "    reached the mex token length (This is also called padding).\n",
    "    List of strings, gensim.models.Word2Vec, Integer -> 3D Numpy array.\n",
    "    '''\n",
    "\n",
    "    vector_length = wordvec_model.vector_size\n",
    "    all_vectors = []\n",
    "\n",
    "    for text in texts:\n",
    "        vector_format = []\n",
    "        tokens = unitok_tokens(text)[0:max_token_length]\n",
    "        for token in tokens:\n",
    "            if token in fin_word2vec_model.raw_vocab:\n",
    "                vector_format.append(fin_word2vec_model[token].reshape(1,vector_length))\n",
    "            else:\n",
    "                vector_format.append(numpy.zeros(300).reshape(1,vector_length))\n",
    "        while len(vector_format) != max_token_length:\n",
    "            vector_format.append(numpy.zeros(vector_length).reshape(1,vector_length))\n",
    "        all_vectors.append(numpy.vstack(vector_format))\n",
    "    return numpy.asarray(all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whitespace_tokens(text):\n",
    "\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyzer(token):\n",
    " \n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(token_list, n_range):\n",
    "    '''Given a list of tokens will return a list of tokens that have been\n",
    "    concatenated with the n closests tokens.'''\n",
    "\n",
    "    def get_n_grams(temp_tokens, n):\n",
    "        token_copy = list(temp_tokens)\n",
    "        gram_tokens = []\n",
    "        while(len(token_copy) >= n):\n",
    "            n_list = []\n",
    "            for i in range(0,n):\n",
    "                n_list.append(token_copy[i])\n",
    "            token_copy.pop(0)\n",
    "            gram_tokens.append(' '.join(n_list))\n",
    "        return gram_tokens\n",
    "\n",
    "    all_n_grams = []\n",
    "    for tokens in token_list:\n",
    "        if n_range == (1,1):\n",
    "            all_n_grams.append(tokens)\n",
    "        else:\n",
    "            all_tokens = []\n",
    "            for n in range(n_range[0], n_range[1] + 1):\n",
    "                all_tokens.extend(get_n_grams(tokens, n))\n",
    "            all_n_grams.append(all_tokens)\n",
    "\n",
    "    return all_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_submitted_values():\n",
    "    early_stop_path = ('Early Stopping',\n",
    "                       config_path(['submitted_data', 'early_stopping']))\n",
    "    tweeked_path = ('Tweeked', config_path(['submitted_data', 'tweeked']))\n",
    "\n",
    "    for sub_name, sub_path in [early_stop_path, tweeked_path]:\n",
    "        sentiment_values = []\n",
    "        with open(sub_path, 'r') as fp:\n",
    "            for data in json.load(fp):\n",
    "                sentiment_values.append(data['sentiment score'])\n",
    "        yield sub_name, sentiment_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_submitted_values():\n",
    "    early_stop_path = ('Early Stopping',\n",
    "                       config_path(['submitted_data', 'early_stopping']))\n",
    "    tweeked_path = ('Tweeked', config_path(['submitted_data', 'tweeked']))\n",
    "\n",
    "    for sub_name, sub_path in [early_stop_path, tweeked_path]:\n",
    "        sentiment_values = []\n",
    "        with open(sub_path, 'r') as fp:\n",
    "            for data in json.load(fp):\n",
    "                sentiment_values.append(data['sentiment score'])\n",
    "        yield sub_name, sentiment_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(predicted_sentiments):\n",
    "\n",
    "\n",
    "    for sub_name, sent_values in __get_submitted_values():\n",
    "        sim_value = 1 - cosine(sent_values, predicted_sentiments)\n",
    "        msg = ('Similarity between your predicted values and {}: {}'\n",
    "              ).format(sub_name, sim_value)\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __text_sentiment_company(all_data):\n",
    "    '''Given a list of dicts will return a tuple of 3 lists containing:\n",
    "    1. list of strings lower cased - text data\n",
    "    2. numpy array (len(text data), 1) dimension of floats - sentiment values\n",
    "    3. list of strings - company names associated to the text data\n",
    "    list of dicts -> tuple(list of strings, numpy array, list of strings)\n",
    "    '''\n",
    "\n",
    "    text = []\n",
    "    sentiment = []\n",
    "    company = []\n",
    "    for data in all_data:\n",
    "        text.append(data['title'].lower())\n",
    "        company.append(data['company'].lower())\n",
    "        # This field does not exist in test dataset\n",
    "        if 'sentiment' in data:\n",
    "            sentiment.append(data['sentiment'])\n",
    "        elif 'sentiment score' in data:\n",
    "            sentiment.append(data['sentiment score'])\n",
    "    return text, numpy.asarray(sentiment), company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fin_data(data_type):\n",
    "    '''Given either train, trail or test string as data type will retrieve\n",
    "    those datasets that were given out in SEMEval task 5 track 2 2017 in the\n",
    "    format of a tuple containing:\n",
    "    1. list of strings lower cased - text data\n",
    "    2. numpy array (len(text data), 1) dimension of floats - sentiment values\n",
    "    3. list of strings - company names associated to the text data\n",
    "    String -> tuple(list of strings, numpy array, list of strings)\n",
    "    '''\n",
    "\n",
    "    #data_path = config_path(['data', 'fin_data', data_type + '_data'])\n",
    "    with open(data_type, 'r') as fp:\n",
    "        return __text_sentiment_company(json.load(fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_score(predicted_values, true_values):\n",
    "    '''Given two arrays of same length returns the cosine similarity where 1\n",
    "    is most similar and 0 is not similar.\n",
    "    list, list -> float\n",
    "    '''\n",
    "\n",
    "    return 1 - cosine(predicted_values, true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats_report(clf, f_name):\n",
    "  \n",
    "    def convert_value(value):\n",
    "\n",
    "        if callable(value):\n",
    "            value = value.__name__\n",
    "        return str(value)\n",
    "\n",
    "    means  = clf.cv_results_['mean_test_score']\n",
    "    stds   = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    with open(f_name, 'w') as fp:\n",
    "        fp.write(\"Mean\\tSD\\t{} \\n\".format('\\t'.join(params[0].keys())))\n",
    "        for mean, std, param in zip(means, stds, params):\n",
    "            param_values = []\n",
    "            for key, value in param.items():\n",
    "                if ('__words_replace' in key or '__disimlar' in key or\n",
    "                    '__word2extract' in key):\n",
    "                    param_values.append(convert_value(value[0]))\n",
    "                else:\n",
    "                    param_values.append(convert_value(value))\n",
    "            fp.write(\"{}\\t{}\\t{}\\n\".format(str(mean), str(std), '\\t'.join(param_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_true_diff(pred_values, true_values, score_function, mapping=None):\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(pred_values)):\n",
    "        mapped_value = i\n",
    "        # This is to support both lists and numpy arrays\n",
    "        if hasattr(mapping,'__index__') or hasattr(mapping, 'index'):\n",
    "            mapped_value = mapping[i]\n",
    "        results.append((mapped_value, pred_values[i],\n",
    "                       score_function([pred_values[i]], [true_values[i]])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_cross_validate(train_data, train_values, model, n_folds=10,\n",
    "                         shuffle=True, score_function=mean_absolute_error):\n",
    " \n",
    "\n",
    "    results = []\n",
    "\n",
    "    train_data_array = numpy.asarray(train_data)\n",
    "    train_values_array = numpy.asarray(train_values)\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=shuffle)\n",
    "    for train, test in kfold.split(train_data_array, train_values_array):\n",
    "        model.fit(train_data_array[train], train_values_array[train])\n",
    "\n",
    "        predicted_values = model.predict(train_data_array[test])\n",
    "        real_values = train_values_array[test]\n",
    "\n",
    "        results.extend(pred_true_diff(predicted_values, real_values,\n",
    "                                      score_function, mapping=test))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_n_errors(error_res, train_data, train_values, companies, n=10):\n",
    "   \n",
    "    \n",
    "\n",
    "    error_res = sorted(error_res, key=lambda value: value[2], reverse=True)\n",
    "    top_errors = error_res[:n]\n",
    "    return [{'Sentence':train_data[index], 'Company':companies[index],\n",
    "            'True value':train_values[index], 'Pred value':pred_value,\n",
    "            'index':index} for index, pred_value, _ in top_errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comps2sent(text_data, companies):\n",
    "  \n",
    "\n",
    "    sentence_compid = {}\n",
    "    for i in range(len(text_data)):\n",
    "        text = text_data[i]\n",
    "        comp = companies[i]\n",
    "        comps_indexs = sentence_compid.get(text, [])\n",
    "        comps_indexs.append((comp,i))\n",
    "        sentence_compid[text] = comps_indexs\n",
    "    compscount_ids = {}\n",
    "    for _, compsid in sentence_compid.items():\n",
    "        ids = compscount_ids.get(len(compsid), [])\n",
    "        ids.append([comp_id[1] for comp_id in compsid])\n",
    "        compscount_ids[len(compsid)] = ids\n",
    "    return compscount_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_type_errors(top_errors, compscount_ids):\n",
    "   \n",
    "\n",
    "    ids_compscount = {}\n",
    "    for compscount, ids_list in compscount_ids.items():\n",
    "        for ids in ids_list:\n",
    "            for a_id in ids:\n",
    "                ids_compscount[a_id] = compscount\n",
    "\n",
    "    comps_errors = {}\n",
    "    for error in top_errors:\n",
    "        sent_id = error['index']\n",
    "        comp_count = ids_compscount[sent_id]\n",
    "        errors = comps_errors.get(comp_count, [])\n",
    "        errors.append(error)\n",
    "        comps_errors[comp_count] = errors\n",
    "    return comps_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_dist(comps_ids):\n",
    "    return {k : len(v) for k, v in comps_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/anaconda2/lib/python2.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel:\n",
    "\n",
    "    def __init__(self, word2vec_model):\n",
    "        self._word2vec_model = word2vec_model\n",
    "        self._max_length = 0\n",
    "        self._model = None\n",
    "\n",
    "\n",
    "\n",
    "    def cross_validate(self, train_text, train_sentiments, n_folds=10,\n",
    "                       shuffle=True, score_function=cosine_score):\n",
    "\n",
    "        all_results = []\n",
    "        train_text_array = numpy.asarray(train_text)\n",
    "        train_sentiments_array = numpy.asarray(train_sentiments)\n",
    "\n",
    "        kfold = KFold(n_splits=n_folds, shuffle=shuffle)\n",
    "        for train, test in kfold.split(train_text_array, train_sentiments_array):\n",
    "            self.fit(train_text_array[train], train_sentiments_array[train])\n",
    "            predicted_sentiments = self.predict(train_text_array[test])\n",
    "            result = score_function(predicted_sentiments, train_sentiments_array[test])\n",
    "            all_results.append(result)\n",
    "        return all_results\n",
    "\n",
    "    def _text2vector(self, texts):\n",
    "        '''Given a list of Strings will convert to a numpy 3D array where each\n",
    "        token in the text is reprsented as a vector from the self.word2vec_model.\n",
    "        see semeval.helper.process_data for more details.\n",
    "        list of strings -> 3D numpy array (len(texts), max_number_tokens,\n",
    "        self.word2vec_model.vector_size)\n",
    "        '''\n",
    "\n",
    "        if self._max_length == 0:\n",
    "            raise Exception('Your model requires training first')\n",
    "\n",
    "        return process_data(texts, self._word2vec_model, self._max_length)\n",
    "\n",
    "    def fit(self):\n",
    "        '''All sub classes should overide this but pre-filter so that random\n",
    "        seed can be set and allow all models to be more reprocible.\n",
    "        '''\n",
    "\n",
    "        # Required for reproducibility\n",
    "        numpy.random.seed(1337)\n",
    "\n",
    "    def predict(self, test_texts):\n",
    "        '''Given a list of strings will return a list of predicted values based\n",
    "        on what the LSTM has been trained on.\n",
    "        List of strings -> list of predicted values.\n",
    "        '''\n",
    "\n",
    "        test_vectors = self._text2vector(test_texts)\n",
    "        if self._model == None:\n",
    "            raise Exception('Your model requires training first')\n",
    "        return self._model.predict(test_vectors)\n",
    "\n",
    "    def _set_max_length(self, texts):\n",
    "\n",
    "        self._max_length = max_length(texts)\n",
    "        return self._max_length\n",
    "\n",
    "    def _set_model(self, model):\n",
    "\n",
    "        self._model = model\n",
    "        return model\n",
    "\n",
    "    def visualise_model(self, f_name):\n",
    "        '''Given a file path will visulaise the LSTM model.\n",
    "        String -> Void\n",
    "        '''\n",
    "\n",
    "        if self._model == None:\n",
    "            raise Exception('Your model requires training first')\n",
    "        plot(self._model, to_file=f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "class EarlyStoppingLSTM(LSTMModel):\n",
    "    '''Model that can train an LSTM and apply the trainned model to unseen\n",
    "    data. Inherits from LSTMModel.\n",
    "    Instance Arguments:\n",
    "    self._word2vec_model - gensim.models.Word2Vec required as an argument to __init__\n",
    "    self._max_length = 0\n",
    "    self._model = None\n",
    "    public methods:\n",
    "    train - trains a Bi-directional LSTM with dropout and early stopping on\n",
    "    the texts and sentiment values given.\n",
    "    test - Using the trained model saved at self._model will return a list of\n",
    "    sentiment values given the texts in the argument of the method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, word2vec_model):\n",
    "        super().__init__(word2vec_model)\n",
    "\n",
    "    def fit(self, train_texts, sentiment_values):\n",
    " \n",
    "\n",
    "        super().fit()\n",
    "\n",
    "        max_length    = self._set_max_length(train_texts)\n",
    "        vector_length = self._word2vec_model.vector_size\n",
    "\n",
    "        train_vectors = self._text2vector(train_texts)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dropout(0.5, input_shape=(max_length, vector_length)))\n",
    "        # Output of this layer is of max_length by max_length * 2 dimension\n",
    "        # instead of max_length, vector_length\n",
    "        model.add(Bidirectional(LSTM(max_length, activation='softsign',\n",
    "                                     return_sequences=True)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Bidirectional(LSTM(max_length, activation='softsign')))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('linear'))\n",
    "\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['cosine_proximity'])\n",
    "                      #clipvalue=5\n",
    "\n",
    "\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "        model.fit(train_vectors, sentiment_values, validation_split=0.1,\n",
    "                  callbacks=[early_stopping] , epochs=100)\n",
    "\n",
    "        return self._set_model(model)\n",
    "    \n",
    "    def predict(self, test_texts):\n",
    "        '''Given a list of strings will return a list of predicted values based\n",
    "        on what the LSTM has been trained on.\n",
    "        List of strings -> list of predicted values.\n",
    "        '''\n",
    "\n",
    "        test_vectors = self._text2vector(test_texts)\n",
    "        if self._model == None:\n",
    "            raise Exception('Your model requires training first')\n",
    "        return self._model.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fin_word_vector():\n",
    "\n",
    "    #fin_word2vec_path = config_path(['models', 'fin_word2vec'])\n",
    "    return gensim.models.Word2Vec.load('all_fin_model_lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts, train_sentiments, train_companies = fin_data(train_data)\n",
    "trial_texts, trial_sentiments, trial_companies = fin_data(trail_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts, test_sentiments, test_companies = fin_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_word2vec_model = fin_word_vector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required to for the results on the test data\n",
    "true_values = eval_format(test_texts, test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_word2vec_model.sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_fin_word2vec_model=gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_format(title_list, sentiment_list):\n",
    "    '''Given a list of strings and a list of floats it will convert them into\n",
    "    a list of dicts so that they can be a parameter in the eval_func.\n",
    "    list of strings, list of floats -> list of dicts\n",
    "    '''\n",
    "\n",
    "    assert len(title_list) == len(sentiment_list), 'The two list have to be of the same length'\n",
    "\n",
    "    return [{'title' : title_list[i], 'sentiment score' : sentiment_list[i]} for\n",
    "            i in range(len(title_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required to for the results on the test data\n",
    "true_values = eval_format(trial_texts, trial_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_lstm = EarlyStoppingLSTM(fin_word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_recursively(text, re_list, depth=0):\n",
    "    if depth >= len(re_list):\n",
    "        return [('*', text)]\n",
    "    token_type, regular_expr = re_list[depth]\n",
    "    tokens = []\n",
    "    pos = 0\n",
    "    while pos < len(text):\n",
    "        m = regular_expr.search(text, pos)\n",
    "        if not m:\n",
    "            tokens.extend(tokenize_recursively(text[pos:], re_list, depth+1))\n",
    "            break\n",
    "        else:\n",
    "            startpos, endpos = m.span()\n",
    "            if startpos > pos:\n",
    "                tokens.extend(tokenize_recursively(text[pos:startpos], re_list, depth+1))\n",
    "            tokens.append((token_type, text[startpos:endpos]))\n",
    "            pos = endpos\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text, configuration):\n",
    "    re_list = configuration.re_list\n",
    "    return tokenize_recursively(text, re_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_analysis(data, values, comps, clf, text=False, cv=None, num_errors=50,\n",
    "                   score_function=mean_absolute_error):\n",
    "\n",
    "    compcount_id = None\n",
    "    if text:\n",
    "        compcount_id = comps2sent(text, comps)\n",
    "    else:\n",
    "        compcount_id = comps2sent(data, comps)\n",
    "    error_results = None\n",
    "    if cv:\n",
    "        if isinstance(cv, dict):\n",
    "            error_results = error_cross_validate(data, values, clf,\n",
    "                                                 score_function=score_function, **cv)\n",
    "        else:\n",
    "            error_results = error_cross_validate(data, values, clf,\n",
    "                                                 score_function=score_function)\n",
    "    else:\n",
    "        pred_values = clf.predict(data)\n",
    "        error_results = pred_true_diff(pred_values, values, score_function)\n",
    "    top_errors = top_n_errors(error_results, data, values,\n",
    "                              comps, n=num_errors)\n",
    "    error_details = sent_type_errors(top_errors, compcount_id)\n",
    "    error_distribution = error_dist(error_details)\n",
    "    return error_details, error_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 924 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "924/924 [==============================] - 3s - loss: 0.1563 - cosine_proximity: -0.0801 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 2/100\n",
      "924/924 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1753 - val_loss: 0.1456 - val_cosine_proximity: 0.1456\n",
      "Epoch 3/100\n",
      "924/924 [==============================] - 3s - loss: 0.1559 - cosine_proximity: -0.0758 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 4/100\n",
      "924/924 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.0714 - val_loss: 0.1428 - val_cosine_proximity: -0.1456\n",
      "Epoch 5/100\n",
      "924/924 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1537 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 6/100\n",
      "924/924 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1061 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 7/100\n",
      "924/924 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1494 - val_loss: 0.1431 - val_cosine_proximity: -0.1456\n",
      "Epoch 8/100\n",
      "924/924 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1364 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 9/100\n",
      "924/924 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1667 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 10/100\n",
      "924/924 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1645 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 11/100\n",
      "924/924 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1494 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 12/100\n",
      "924/924 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1234 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Epoch 13/100\n",
      "924/924 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1667 - val_loss: 0.1431 - val_cosine_proximity: -0.1456\n",
      "Epoch 14/100\n",
      "924/924 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1385 - val_loss: 0.1433 - val_cosine_proximity: -0.1456\n",
      "Epoch 15/100\n",
      "924/924 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1645 - val_loss: 0.1429 - val_cosine_proximity: -0.1456\n",
      "Train on 924 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "924/924 [==============================] - 2s - loss: 0.1571 - cosine_proximity: -0.0617 - val_loss: 0.1568 - val_cosine_proximity: -0.1262\n",
      "Epoch 2/100\n",
      "924/924 [==============================] - 2s - loss: 0.1570 - cosine_proximity: -0.1786 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 3/100\n",
      "924/924 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1872 - val_loss: 0.1562 - val_cosine_proximity: -0.12620.189\n",
      "Epoch 4/100\n",
      "924/924 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1959 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 5/100\n",
      "924/924 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1764 - val_loss: 0.1568 - val_cosine_proximity: -0.1262\n",
      "Epoch 6/100\n",
      "924/924 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1548 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 7/100\n",
      "924/924 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1461 - val_loss: 0.1571 - val_cosine_proximity: -0.1262\n",
      "Epoch 8/100\n",
      "924/924 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1959 - val_loss: 0.1567 - val_cosine_proximity: -0.12620.129 - ETA: 1s - loss: 0.1610 - cosine_pro\n",
      "Epoch 9/100\n",
      "924/924 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1461 - val_loss: 0.1562 - val_cosine_proximity: -0.1262\n",
      "Epoch 10/100\n",
      "924/924 [==============================] - 2s - loss: 0.1576 - cosine_proximity: -0.1353 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 11/100\n",
      "924/924 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1504 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 12/100\n",
      "924/924 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1786 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 13/100\n",
      "924/924 [==============================] - 1s - loss: 0.1564 - cosine_proximity: -0.1591 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 14/100\n",
      "924/924 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1807 - val_loss: 0.1562 - val_cosine_proximity: -0.1262\n",
      "Epoch 15/100\n",
      "924/924 [==============================] - 1s - loss: 0.1565 - cosine_proximity: -0.1569 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 16/100\n",
      "924/924 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1569 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 17/100\n",
      "924/924 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1699 - val_loss: 0.1568 - val_cosine_proximity: -0.12620.1 - ETA: 1s - loss: 0.1432 - cosine\n",
      "Epoch 18/100\n",
      "924/924 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1764 - val_loss: 0.1563 - val_cosine_proximity: -0.1262\n",
      "Epoch 19/100\n",
      "924/924 [==============================] - 1s - loss: 0.1562 - cosine_proximity: -0.1613 - val_loss: 0.1562 - val_cosine_proximity: -0.1262\n",
      "Epoch 20/100\n",
      "924/924 [==============================] - 1s - loss: 0.1571 - cosine_proximity: -0.1548 - val_loss: 0.1564 - val_cosine_proximity: -0.1262\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1570 - cosine_proximity: -0.1914 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1571 - cosine_proximity: -0.1665 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1535 - val_loss: 0.1393 - val_cosine_proximity: -0.1456\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1859 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.2011 - val_loss: 0.1393 - val_cosine_proximity: -0.1456\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1773 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1730 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1730 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1751 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1665 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1622 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.2076 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1514 - val_loss: 0.1394 - val_cosine_proximity: -0.1456\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1730 - val_loss: 0.1391 - val_cosine_proximity: -0.1456\n",
      "Epoch 15/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1578 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 16/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1946 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 17/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1859 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 18/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.2032 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 19/100\n",
      "925/925 [==============================] - 2s - loss: 0.1571 - cosine_proximity: -0.1751 - val_loss: 0.1393 - val_cosine_proximity: -0.1456\n",
      "Epoch 20/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1838 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 21/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1946 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 22/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1816 - val_loss: 0.1391 - val_cosine_proximity: -0.1456\n",
      "Epoch 23/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1751 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 24/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1557 - val_loss: 0.1394 - val_cosine_proximity: -0.1456\n",
      "Epoch 25/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1859 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 26/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1730 - val_loss: 0.1394 - val_cosine_proximity: -0.1456\n",
      "Epoch 27/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1924 - val_loss: 0.1393 - val_cosine_proximity: -0.1456\n",
      "Epoch 28/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1946 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 29/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1816 - val_loss: 0.1393 - val_cosine_proximity: -0.1456\n",
      "Epoch 30/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1924 - val_loss: 0.1394 - val_cosine_proximity: -0.1456\n",
      "Epoch 31/100\n",
      "925/925 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1968 - val_loss: 0.1399 - val_cosine_proximity: -0.1456\n",
      "Epoch 32/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1730 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Epoch 33/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1859 - val_loss: 0.1392 - val_cosine_proximity: -0.1456\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1554 - cosine_proximity: -0.0832 - val_loss: 0.1473 - val_cosine_proximity: -0.1650\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - ETA: 0s - loss: 0.1566 - cosine_proximity: -0.132 - 2s - loss: 0.1550 - cosine_proximity: -0.1200 - val_loss: 0.1480 - val_cosine_proximity: -0.1650\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.0876 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1308 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1550 - cosine_proximity: -0.1697 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1395 - val_loss: 0.1474 - val_cosine_proximity: -0.1650\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1200 - val_loss: 0.1475 - val_cosine_proximity: -0.1650\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1373 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1676 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1547 - cosine_proximity: -0.1351 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1524 - val_loss: 0.1473 - val_cosine_proximity: -0.1650\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1589 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1676 - val_loss: 0.1473 - val_cosine_proximity: -0.1650\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1551 - cosine_proximity: -0.1265 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 15/100\n",
      "925/925 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1481 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 16/100\n",
      "925/925 [==============================] - 2s - loss: 0.1550 - cosine_proximity: -0.1611 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 17/100\n",
      "925/925 [==============================] - 2s - loss: 0.1550 - cosine_proximity: -0.1416 - val_loss: 0.1474 - val_cosine_proximity: -0.1650\n",
      "Epoch 18/100\n",
      "925/925 [==============================] - 2s - loss: 0.1547 - cosine_proximity: -0.1416 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 19/100\n",
      "925/925 [==============================] - 2s - loss: 0.1545 - cosine_proximity: -0.1114 - val_loss: 0.1473 - val_cosine_proximity: -0.1650\n",
      "Epoch 20/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.0984 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 21/100\n",
      "925/925 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1157 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 22/100\n",
      "925/925 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1395 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 23/100\n",
      "925/925 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1286 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Epoch 24/100\n",
      "925/925 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1459 - val_loss: 0.1472 - val_cosine_proximity: -0.1650\n",
      "Epoch 25/100\n",
      "925/925 [==============================] - 2s - loss: 0.1549 - cosine_proximity: -0.1503 - val_loss: 0.1471 - val_cosine_proximity: -0.1650\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1572 - cosine_proximity: -0.0951 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1232 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1665 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1665 - val_loss: 0.1515 - val_cosine_proximity: -0.1553\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1622 - val_loss: 0.1514 - val_cosine_proximity: -0.1553\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1751 - val_loss: 0.1514 - val_cosine_proximity: -0.1553\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1435 - val_loss: 0.1514 - val_cosine_proximity: -0.1553\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1751 - val_loss: 0.1515 - val_cosine_proximity: -0.1553\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1254 - val_loss: 0.1515 - val_cosine_proximity: -0.1553\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1570 - cosine_proximity: -0.1795 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1643 - val_loss: 0.1515 - val_cosine_proximity: -0.1553\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1686 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1643 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1319 - val_loss: 0.1514 - val_cosine_proximity: -0.15530. - ETA: 0s - loss: 0.1594 - cosine_proximity: -0.1 - ETA: 0s - loss: 0.1586 - cosine_proximity: -0.\n",
      "Epoch 15/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1708 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 16/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1427 - val_loss: 0.1514 - val_cosine_proximity: -0.1553\n",
      "Epoch 17/100\n",
      "925/925 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1751 - val_loss: 0.1514 - val_cosine_proximity: -0.1553\n",
      "Epoch 18/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1665 - val_loss: 0.1514 - val_cosine_proximity: -0.1553\n",
      "Epoch 19/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1449 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 20/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1578 - val_loss: 0.1513 - val_cosine_proximity: -0.1553\n",
      "Epoch 21/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1557 - val_loss: 0.1514 - val_cosine_proximity: -0.15530.\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1556 - cosine_proximity: -0.1351 - val_loss: 0.1576 - val_cosine_proximity: -0.1845\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1549 - cosine_proximity: -0.1935 - val_loss: 0.1576 - val_cosine_proximity: -0.1845\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1548 - cosine_proximity: -0.1719 - val_loss: 0.1577 - val_cosine_proximity: -0.1845\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1222 - val_loss: 0.1580 - val_cosine_proximity: -0.1845\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1805 - val_loss: 0.1576 - val_cosine_proximity: -0.1845\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1200 - val_loss: 0.1577 - val_cosine_proximity: -0.1845\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1676 - val_loss: 0.1577 - val_cosine_proximity: -0.1845\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1546 - val_loss: 0.1584 - val_cosine_proximity: -0.1845\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1551 - cosine_proximity: -0.1611 - val_loss: 0.1580 - val_cosine_proximity: -0.1845\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1568 - val_loss: 0.1576 - val_cosine_proximity: -0.1845\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1550 - cosine_proximity: -0.1611 - val_loss: 0.1577 - val_cosine_proximity: -0.1845\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1548 - cosine_proximity: -0.1654 - val_loss: 0.1576 - val_cosine_proximity: -0.1845\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1546 - val_loss: 0.1576 - val_cosine_proximity: -0.1845\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1562 - cosine_proximity: -0.1481 - val_loss: 0.1548 - val_cosine_proximity: -0.1650\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1481 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1676 - val_loss: 0.1544 - val_cosine_proximity: -0.1650\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1438 - val_loss: 0.1546 - val_cosine_proximity: -0.1650\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1351 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1503 - val_loss: 0.1547 - val_cosine_proximity: -0.1650\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1676 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1524 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1416 - val_loss: 0.1545 - val_cosine_proximity: -0.1650\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1243 - val_loss: 0.1552 - val_cosine_proximity: -0.16500.137\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.0811 - val_loss: 0.1544 - val_cosine_proximity: -0.1650\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1157 - val_loss: 0.1547 - val_cosine_proximity: -0.1650\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1416 - val_loss: 0.1546 - val_cosine_proximity: -0.1650\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1503 - val_loss: 0.1549 - val_cosine_proximity: -0.1650\n",
      "Epoch 15/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1373 - val_loss: 0.1545 - val_cosine_proximity: -0.1650\n",
      "Epoch 16/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1395 - val_loss: 0.1544 - val_cosine_proximity: -0.1650\n",
      "Epoch 17/100\n",
      "925/925 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1459 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 18/100\n",
      "925/925 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1676 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 19/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1265 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 20/100\n",
      "925/925 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1092 - val_loss: 0.1544 - val_cosine_proximity: -0.1650\n",
      "Epoch 21/100\n",
      "925/925 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1524 - val_loss: 0.1548 - val_cosine_proximity: -0.1650\n",
      "Epoch 22/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1265 - val_loss: 0.1548 - val_cosine_proximity: -0.1650\n",
      "Epoch 23/100\n",
      "925/925 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1005 - val_loss: 0.1547 - val_cosine_proximity: -0.1650\n",
      "Epoch 24/100\n",
      "925/925 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1589 - val_loss: 0.1544 - val_cosine_proximity: -0.1650\n",
      "Epoch 25/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1308 - val_loss: 0.1544 - val_cosine_proximity: -0.1650\n",
      "Epoch 26/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1676 - val_loss: 0.1548 - val_cosine_proximity: -0.1650\n",
      "Epoch 27/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.0659 - val_loss: 0.1543 - val_cosine_proximity: -0.1650\n",
      "Epoch 28/100\n",
      "925/925 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1719 - val_loss: 0.1545 - val_cosine_proximity: -0.1650\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1565 - cosine_proximity: -0.1070 - val_loss: 0.1570 - val_cosine_proximity: -0.1068\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1384 - val_loss: 0.1574 - val_cosine_proximity: -0.1068\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1576 - cosine_proximity: -0.1795 - val_loss: 0.1571 - val_cosine_proximity: -0.1068\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1795 - val_loss: 0.1569 - val_cosine_proximity: -0.1068\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1514 - val_loss: 0.1573 - val_cosine_proximity: -0.1068\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1730 - val_loss: 0.1570 - val_cosine_proximity: -0.1068\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1492 - val_loss: 0.1572 - val_cosine_proximity: -0.1068\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1567 - cosine_proximity: -0.1730 - val_loss: 0.1572 - val_cosine_proximity: -0.1068\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1427 - val_loss: 0.1569 - val_cosine_proximity: -0.10680.137\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1124 - val_loss: 0.1570 - val_cosine_proximity: -0.1068\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1492 - val_loss: 0.1571 - val_cosine_proximity: -0.1068\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1569 - cosine_proximity: -0.1557 - val_loss: 0.1570 - val_cosine_proximity: -0.1068\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1570 - cosine_proximity: -0.1903 - val_loss: 0.1570 - val_cosine_proximity: -0.1068\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1573 - cosine_proximity: -0.1470 - val_loss: 0.1572 - val_cosine_proximity: -0.1068\n",
      "Epoch 15/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1384 - val_loss: 0.1570 - val_cosine_proximity: -0.1068\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1568 - cosine_proximity: -0.1524 - val_loss: 0.1464 - val_cosine_proximity: -0.1942\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1459 - val_loss: 0.1467 - val_cosine_proximity: -0.1942\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1719 - val_loss: 0.1457 - val_cosine_proximity: -0.1942\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1395 - val_loss: 0.1482 - val_cosine_proximity: -0.1942\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1568 - val_loss: 0.1469 - val_cosine_proximity: -0.1942\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1575 - cosine_proximity: -0.1438 - val_loss: 0.1466 - val_cosine_proximity: -0.1942\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1481 - val_loss: 0.1464 - val_cosine_proximity: -0.1942\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1568 - cosine_proximity: -0.1654 - val_loss: 0.1466 - val_cosine_proximity: -0.1942\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1719 - val_loss: 0.1462 - val_cosine_proximity: -0.1942\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1719 - val_loss: 0.1460 - val_cosine_proximity: -0.1942\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1741 - val_loss: 0.1459 - val_cosine_proximity: -0.1942\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1805 - val_loss: 0.1468 - val_cosine_proximity: -0.1942\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.0962 - val_loss: 0.1462 - val_cosine_proximity: -0.1942\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1572 - cosine_proximity: -0.1351 - val_loss: 0.1459 - val_cosine_proximity: -0.1942\n",
      "Train on 925 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "925/925 [==============================] - 3s - loss: 0.1546 - cosine_proximity: -0.1622 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 2/100\n",
      "925/925 [==============================] - 2s - loss: 0.1547 - cosine_proximity: -0.1849 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 3/100\n",
      "925/925 [==============================] - 2s - loss: 0.1537 - cosine_proximity: -0.1805 - val_loss: 0.1576 - val_cosine_proximity: -0.1456\n",
      "Epoch 4/100\n",
      "925/925 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1654 - val_loss: 0.1572 - val_cosine_proximity: -0.1456\n",
      "Epoch 5/100\n",
      "925/925 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1741 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 6/100\n",
      "925/925 [==============================] - 2s - loss: 0.1540 - cosine_proximity: -0.1654 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 7/100\n",
      "925/925 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1351 - val_loss: 0.1572 - val_cosine_proximity: -0.1456\n",
      "Epoch 8/100\n",
      "925/925 [==============================] - 2s - loss: 0.1547 - cosine_proximity: -0.1503 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 9/100\n",
      "925/925 [==============================] - 2s - loss: 0.1546 - cosine_proximity: -0.1459 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 10/100\n",
      "925/925 [==============================] - 2s - loss: 0.1540 - cosine_proximity: -0.1805 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 11/100\n",
      "925/925 [==============================] - 2s - loss: 0.1547 - cosine_proximity: -0.1676 - val_loss: 0.1572 - val_cosine_proximity: -0.1456\n",
      "Epoch 12/100\n",
      "925/925 [==============================] - 2s - loss: 0.1539 - cosine_proximity: -0.1827 - val_loss: 0.1571 - val_cosine_proximity: -0.14560.\n",
      "Epoch 13/100\n",
      "925/925 [==============================] - 2s - loss: 0.1540 - cosine_proximity: -0.1805 - val_loss: 0.1570 - val_cosine_proximity: -0.1456\n",
      "Epoch 14/100\n",
      "925/925 [==============================] - 2s - loss: 0.1537 - cosine_proximity: -0.1719 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 15/100\n",
      "925/925 [==============================] - 2s - loss: 0.1545 - cosine_proximity: -0.1849 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 16/100\n",
      "925/925 [==============================] - 2s - loss: 0.1541 - cosine_proximity: -0.1784 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 17/100\n",
      "925/925 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1741 - val_loss: 0.1570 - val_cosine_proximity: -0.1456\n",
      "Epoch 18/100\n",
      "925/925 [==============================] - 2s - loss: 0.1545 - cosine_proximity: -0.1632 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 19/100\n",
      "925/925 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1546 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 20/100\n",
      "925/925 [==============================] - 2s - loss: 0.1548 - cosine_proximity: -0.1784 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 21/100\n",
      "925/925 [==============================] - 2s - loss: 0.1538 - cosine_proximity: -0.1827 - val_loss: 0.1573 - val_cosine_proximity: -0.1456\n",
      "Epoch 22/100\n",
      "925/925 [==============================] - 2s - loss: 0.1541 - cosine_proximity: -0.1416 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 23/100\n",
      "925/925 [==============================] - 2s - loss: 0.1539 - cosine_proximity: -0.1676 - val_loss: 0.1571 - val_cosine_proximity: -0.1456\n",
      "Epoch 24/100\n",
      "925/925 [==============================] - 2s - loss: 0.1544 - cosine_proximity: -0.1827 - val_loss: 0.1570 - val_cosine_proximity: -0.1456\n"
     ]
    }
   ],
   "source": [
    "# Get the 10 fold cross validation results\n",
    "early_res = early_lstm.cross_validate(train_texts, train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1027 samples, validate on 115 samples\n",
      "Epoch 1/100\n",
      "1027/1027 [==============================] - 4s - loss: 0.1568 - cosine_proximity: -0.1130 - val_loss: 0.1507 - val_cosine_proximity: -0.1478\n",
      "Epoch 2/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1548 - val_loss: 0.1512 - val_cosine_proximity: -0.1478\n",
      "Epoch 3/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1743 - val_loss: 0.1508 - val_cosine_proximity: -0.1478\n",
      "Epoch 4/100\n",
      "1027/1027 [==============================] - 3s - loss: 0.1559 - cosine_proximity: -0.1646 - val_loss: 0.1509 - val_cosine_proximity: -0.1478\n",
      "Epoch 5/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1685 - val_loss: 0.1508 - val_cosine_proximity: -0.1478\n",
      "Epoch 6/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1568 - val_loss: 0.1510 - val_cosine_proximity: -0.1478\n",
      "Epoch 7/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1587 - val_loss: 0.1507 - val_cosine_proximity: -0.1478\n",
      "Epoch 8/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1685 - val_loss: 0.1509 - val_cosine_proximity: -0.1478\n",
      "Epoch 9/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1607 - val_loss: 0.1508 - val_cosine_proximity: -0.1478\n",
      "Epoch 10/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1782 - val_loss: 0.1511 - val_cosine_proximity: -0.1478\n",
      "Epoch 11/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1782 - val_loss: 0.1508 - val_cosine_proximity: -0.1478\n",
      "Epoch 12/100\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1334 - val_loss: 0.1508 - val_cosine_proximity: -0.1478\n"
     ]
    }
   ],
   "source": [
    "early_lstm.fit(train_texts, train_sentiments)\n",
    "early_error_details, early_error_dist = error_analysis(test_texts, test_sentiments,\n",
    "                                                            test_companies, early_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [{'Company': 'bp',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'bp reports biggest ever annual loss',\n",
       "   'True value': -0.98999999999999999,\n",
       "   'index': 415},\n",
       "  {'Company': 'glencore',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'glencore shares in record crash as profit fears grow',\n",
       "   'True value': -0.97099999999999997,\n",
       "   'index': 142},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'oil giant bp reports loss of $4.4 billion in 4th quarter of 2014',\n",
       "   'True value': -0.96299999999999997,\n",
       "   'index': 83},\n",
       "  {'Company': 'aviva plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'aviva posts forecast-beating 2015 operating profit of $3.8 bln',\n",
       "   'True value': 0.94599999999999995,\n",
       "   'index': 257},\n",
       "  {'Company': 'persimmon',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'update: persimmon profit up strongly, outlook positive',\n",
       "   'True value': 0.93700000000000006,\n",
       "   'index': 442},\n",
       "  {'Company': 'morrisons',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'companiesmorrison evicted from ftse 100 as worldpay joins',\n",
       "   'True value': -0.85199999999999998,\n",
       "   'index': 31},\n",
       "  {'Company': 'old mutual',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'old mutual shares soar on break-up report',\n",
       "   'True value': 0.91100000000000003,\n",
       "   'index': 327},\n",
       "  {'Company': 'volkswagen',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'us subpoenas volkswagen under bank fraud law in emissions probe: source',\n",
       "   'True value': -0.83799999999999997,\n",
       "   'index': 149},\n",
       "  {'Company': 'hsbc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'hsbc reaches $470 million accord over foreclosure abuses',\n",
       "   'True value': -0.83499999999999996,\n",
       "   'index': 320},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'barclays faces another heavy forex fine',\n",
       "   'True value': -0.83399999999999996,\n",
       "   'index': 4},\n",
       "  {'Company': 'weir group',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'slump in weir leads ftse down from record high',\n",
       "   'True value': -0.82699999999999996,\n",
       "   'index': 284},\n",
       "  {'Company': 'm&g investments',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'aviva, m&g suspend property funds as investors panic',\n",
       "   'True value': -0.80700000000000005,\n",
       "   'index': 80},\n",
       "  {'Company': 'fresnillo',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'companiesfresnillo shares jump 8% as silver price breaks $21',\n",
       "   'True value': 0.874,\n",
       "   'index': 394},\n",
       "  {'Company': 'volkswagen',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'germany lags european share rally as volkswagen scandal widens',\n",
       "   'True value': -0.80300000000000005,\n",
       "   'index': 249},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'copper falls with mining stocks as barclays warns of pullback',\n",
       "   'True value': -0.80000000000000004,\n",
       "   'index': 487},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'foreign exchange probes push rbs to â£446m loss',\n",
       "   'True value': -0.79100000000000004,\n",
       "   'index': 479},\n",
       "  {'Company': 'wolseley',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'marketswolseley shares wilt 8.8% after full year results',\n",
       "   'True value': -0.78700000000000003,\n",
       "   'index': 158},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'warning as rbs slumps to â£446m loss',\n",
       "   'True value': -0.78200000000000003,\n",
       "   'index': 303},\n",
       "  {'Company': 'glencore',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'glencore slumps 30 percent as debt fears grow',\n",
       "   'True value': -0.78100000000000003,\n",
       "   'index': 280},\n",
       "  {'Company': 'tesco plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'tesco share price tumbles after negative broker note',\n",
       "   'True value': -0.77900000000000003,\n",
       "   'index': 445},\n",
       "  {'Company': 'london stock exchange group',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'news feedftse 100 movers: lse surges as ice says mulling offer; ashtead and barclays tank',\n",
       "   'True value': 0.84299999999999997,\n",
       "   'index': 386},\n",
       "  {'Company': 'shire',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'homebuilders, shire gains spur ftse 100 to rally beyond record',\n",
       "   'True value': 0.83699999999999997,\n",
       "   'index': 11},\n",
       "  {'Company': 'bhp billiton',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'update 2-bhp billiton profit dives to 10-year low on commodities rout',\n",
       "   'True value': -0.754,\n",
       "   'index': 381},\n",
       "  {'Company': 'morrisons',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'market report: new morrisons chief executive buys another â£500000 worth of ...',\n",
       "   'True value': 0.82199999999999995,\n",
       "   'index': 41},\n",
       "  {'Company': 'wilshire bancorp',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'bbcn bancorp to buy wilshire bancorp in $1 bln deal',\n",
       "   'True value': 0.81799999999999995,\n",
       "   'index': 363},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'update 2-three former barclays traders found guilty in libor rigging trial',\n",
       "   'True value': -0.74299999999999999,\n",
       "   'index': 49},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'bp to slash 4000 jobs globally as oil prices drop',\n",
       "   'True value': -0.73599999999999999,\n",
       "   'index': 178},\n",
       "  {'Company': 'intertek group',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': \"intertek swings to â£347 mln loss on oil's slump\",\n",
       "   'True value': -0.73399999999999999,\n",
       "   'index': 293},\n",
       "  {'Company': 'morrisons',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'industry newsmorrisons unveils which stores will close as 900 jobs face the axe',\n",
       "   'True value': -0.73099999999999998,\n",
       "   'index': 456},\n",
       "  {'Company': 'glencore',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'companiesglencore shares hit 3-month high after refinancing key credit line',\n",
       "   'True value': 0.79500000000000004,\n",
       "   'index': 25},\n",
       "  {'Company': 'standard life',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'property stocks slump after standard life freezes u.k. fund',\n",
       "   'True value': -0.71799999999999997,\n",
       "   'index': 136},\n",
       "  {'Company': 'antofagasta',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'industry newsstrong end to a tough year for antofagasta',\n",
       "   'True value': 0.77100000000000002,\n",
       "   'index': 59},\n",
       "  {'Company': 'lloyds banking group plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'lloyds banking group reports 7% dip in annual profits',\n",
       "   'True value': -0.69599999999999995,\n",
       "   'index': 432},\n",
       "  {'Company': 'j sainsbury plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'why i would put j sainsbury plc in my trolley before wm morrison supermarkets ...',\n",
       "   'True value': 0.76400000000000001,\n",
       "   'index': 323},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'barclays shares dive as plan for ring-fencing costs extra â£1 billion',\n",
       "   'True value': -0.69299999999999995,\n",
       "   'index': 387},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'judge: 22 bp fraud charges against lawyer duplicate others',\n",
       "   'True value': -0.67800000000000005,\n",
       "   'index': 173},\n",
       "  {'Company': 'direct line insurance group plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'direct line raises 2015 guidance on combined ratio improvement',\n",
       "   'True value': 0.746,\n",
       "   'index': 166},\n",
       "  {'Company': 'london stock exchange group',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'london stock exchange boosts ftse on new bid hopes',\n",
       "   'True value': 0.74299999999999999,\n",
       "   'index': 312},\n",
       "  {'Company': 'land securities',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'land securities share price: group reveals strong full-year results',\n",
       "   'True value': 0.73999999999999999,\n",
       "   'index': 314},\n",
       "  {'Company': 'acerta',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'royal mail fined â£40m after french probe',\n",
       "   'True value': -0.67100000000000004,\n",
       "   'index': 79},\n",
       "  {'Company': 'sunray',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'sunray cuts jobs as new hampshire reaches cap on solar incentive',\n",
       "   'True value': -0.67000000000000004,\n",
       "   'index': 338},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': \"industry newsbarclays to 'cut investment bank staff by another 20%'\",\n",
       "   'True value': -0.66700000000000004,\n",
       "   'index': 261},\n",
       "  {'Company': 'old mutual',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'rpt-old mutual q1 gross sales beat forecasts, up 18 pct',\n",
       "   'True value': 0.72399999999999998,\n",
       "   'index': 91},\n",
       "  {'Company': 'actelion',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'companiesactelion shares hit record on shire takeover talk',\n",
       "   'True value': 0.72099999999999997,\n",
       "   'index': 481},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': \"how big was bp's $5.8 billion loss?\",\n",
       "   'True value': -0.65100000000000002,\n",
       "   'index': 65},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'rbs shares drop after surprise pension charge cuts capital',\n",
       "   'True value': -0.65100000000000002,\n",
       "   'index': 439},\n",
       "  {'Company': 'bhp billiton',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'miners meltdown as bhp to rio tinto sink in commodities rout',\n",
       "   'True value': -0.64600000000000002,\n",
       "   'index': 379}],\n",
       " 2: [{'Company': 'astrazeneca',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'why astrazeneca plc & dixons carphone plc are red-hot growth stars!',\n",
       "   'True value': 0.93400000000000005,\n",
       "   'index': 245},\n",
       "  {'Company': 'dixons carphone',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': 'why astrazeneca plc & dixons carphone plc are red-hot growth stars!',\n",
       "   'True value': 0.76600000000000001,\n",
       "   'index': 437},\n",
       "  {'Company': 'prudential',\n",
       "   'Pred value': array([ 0.03432425], dtype=float32),\n",
       "   'Sentence': \"uk's ftse has worst day so far in 2015 as bg and prudential fall\",\n",
       "   'True value': -0.65100000000000002,\n",
       "   'index': 2}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_error_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 47, 2: 3}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_error_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric3(pred_values, true_values):\n",
    "    '''Given two lists finds the similarities between the list using the\n",
    "    equation 5 on slide 20 of the presentation at:\n",
    "    ./presentation/slides.pdf\n",
    "    List of ints, List of ints -> int\n",
    "    '''\n",
    "\n",
    "    all_score = 0\n",
    "    if len(pred_values) > 1:\n",
    "        cosine_value = cosine_score(numpy.asarray(pred_values),\n",
    "                                    numpy.asarray(true_values))\n",
    "        if numpy.isnan(cosine_value):\n",
    "            cosine_value = 0\n",
    "        all_score = len(pred_values) * cosine_value\n",
    "\n",
    "    if len(pred_values) == 1:\n",
    "        pred_score = pred_values[0]\n",
    "        test_score = true_values[0]\n",
    "        if pred_score==0 and test_score==0:\n",
    "            all_score = 1\n",
    "        elif test_score==0 or (pred_score / test_score) > 0:\n",
    "            all_score = 1 - math.fabs(true_values[0] - pred_values[0])\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_func(test_data, pred_data, metric=metric3):\n",
    "    '''Takes a list of dicts where each dict contains two keys:\n",
    "    'sentiment score' - a float value\n",
    "    'title' - a string\n",
    "    The function finds the mean cosine similarity between each titles sentiment values.\n",
    "    (A title can have more than one sentiment value associated with it if it has more\n",
    "    than one company mentioned.)\n",
    "    Optional argument:\n",
    "    metric - is a function which defines the metric that you would like to use.\n",
    "    See the metric functions within this module.\n",
    "    Default is metric3\n",
    "    List of dicts, list of dicts -> float\n",
    "    '''\n",
    "\n",
    "    all_vals   = []\n",
    "    title_id   = {}\n",
    "    test_sents = []\n",
    "    pred_sents = []\n",
    "    for i in range(len(test_data)):\n",
    "        data = test_data[i]\n",
    "        ids = title_id.get(data['title'], [])\n",
    "        ids.append(i)\n",
    "        title_id[data['title']] = ids\n",
    "        test_sents.append(test_data[i]['sentiment score'])\n",
    "        pred_sents.append(pred_data[i]['sentiment score'])\n",
    "\n",
    "    if metric == metric1:\n",
    "        return metric1(pred_sents, test_sents)\n",
    "\n",
    "    for _, ids in title_id.items():\n",
    "\n",
    "        pred_sent_scores = []\n",
    "        test_sent_scores = []\n",
    "        for a_id in ids:\n",
    "            pred_value = pred_data[a_id]['sentiment score']\n",
    "            test_value = test_data[a_id]['sentiment score']\n",
    "\n",
    "            pred_sent_scores.append(pred_value)\n",
    "            test_sent_scores.append(test_value)\n",
    "\n",
    "        all_vals.append(metric(pred_sent_scores, test_sent_scores))\n",
    "\n",
    "    if metric == metric2:\n",
    "        return sum(all_vals) / len(all_vals)\n",
    "    elif metric == metric3:\n",
    "        return sum(all_vals) / len(test_data)\n",
    "    else:\n",
    "        raise Exception('Cannot identify that metric function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric1(pred_values, test_values):\n",
    "    '''Wrapper for cosine_score, given two lists returns an int.\n",
    "    Wrapper so that the function name matches the name in the presentation and\n",
    "    paper.\n",
    "    List of ints, List of ints -> int\n",
    "    '''\n",
    "\n",
    "    return cosine_score(pred_values, test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric2(pred_values, test_values):\n",
    "    '''Given two lists finds the similarities between the list using the\n",
    "    equation 4 on slide 20 of the presentation at:\n",
    "    ./presentation/slides.pdf\n",
    "    List of ints, List of ints -> int\n",
    "    '''\n",
    "\n",
    "    all_score = 0\n",
    "    cosine_value = cosine_score(numpy.asarray(pred_values),\n",
    "                                numpy.asarray(test_values))\n",
    "    if not numpy.isnan(cosine_value):\n",
    "        all_score = cosine_value\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric 1 0.03579421126901983\n",
      "Metric 2 0.12382685130371796\n",
      "Metric 3 0.41390099818946685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    }
   ],
   "source": [
    "pred_values = eval_format(test_texts, early_lstm.predict(test_texts))\n",
    "\n",
    "print('Metric 1 {}'.format(eval_func(true_values, pred_values, metric=metric1)))\n",
    "print('Metric 2 {}'.format(eval_func(true_values, pred_values, metric=metric2)))\n",
    "print('Metric 3 {}'.format(eval_func(true_values, pred_values, metric=metric3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tweaked LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TweekedLSTM(LSTMModel):\n",
    "    '''Model that can train an LSTM and apply the trainned model to unseen\n",
    "    data. Inherits from LSTMModel.\n",
    "    Instance Arguments:\n",
    "    self._word2vec_model - gensim.models.Word2Vec required as an argument to __init__\n",
    "    self._max_length = 0\n",
    "    self._model = None\n",
    "    public methods:\n",
    "    train - trains a Bi-directional LSTM with dropout and manually set stopping\n",
    "    on the texts and sentiment values given.\n",
    "    test - Using the trained model saved at self._model will return a list of\n",
    "    sentiment values given the texts in the argument of the method.\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, word2vec_model):\n",
    "        super().__init__(word2vec_model)\n",
    "\n",
    "    def fit(self, train_texts, sentiment_values):\n",
    "        '''Given a list of Strings and a list of floats (sentiments) or numpy\n",
    "        array of floats. It will return a trained LSTM model and `save` the model to\n",
    "        self._model for future use using self.test(texts).\n",
    "        The model converts the list of strings into list of numpy matrixs\n",
    "        which has the following dimensions:\n",
    "        length of the longest train text broken down into tokens\n",
    "        by\n",
    "        the vector size of the word2vec model given in the constructor\n",
    "        e.g. 21, 300 if the word2vec model vector size if 300 and the length of\n",
    "        the longest train text in tokens is 21.\n",
    "        For more details on the layers use read the source or after training\n",
    "        visualise using visualise_model function.\n",
    "        '''\n",
    "\n",
    "        super().fit()\n",
    "\n",
    "        # Required for any transformation of text latter.\n",
    "        max_length    = self._set_max_length(train_texts)\n",
    "        vector_length = self._word2vec_model.vector_size\n",
    "\n",
    "        train_vectors = self._text2vector(train_texts)\n",
    "\n",
    "        model = Sequential()\n",
    "        # Output of this layer is of max_length by max_length * 2 dimension\n",
    "        # instead of max_length, vector_length\n",
    "        model.add(Bidirectional(LSTM(max_length, activation='softsign',\n",
    "                                     dropout_W=0.2, dropout_U=0.2,\n",
    "                                     return_sequences=True),\n",
    "                                input_shape=(max_length, vector_length)))\n",
    "        model.add(Bidirectional(LSTM(max_length, activation='softsign',\n",
    "                                     dropout_W=0.2, dropout_U=0.2)))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('linear'))\n",
    "\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['cosine_proximity'])\n",
    "                      #clipvalue=5\n",
    "\n",
    "        model.fit(train_vectors, sentiment_values, nb_epoch=25)\n",
    "\n",
    "        return self._set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(21, recurrent_dropout=0.2, dropout=0.2, activation=\"softsign\", return_sequences=True)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:51: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(21, recurrent_dropout=0.2, dropout=0.2, activation=\"softsign\")`\n",
      "/usr/local/lib/python3.5/dist-packages/Keras-2.0.8-py3.5.egg/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1027/1027 [==============================] - 4s - loss: 0.1558 - cosine_proximity: -0.1529     \n",
      "Epoch 2/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1675     \n",
      "Epoch 3/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1792     \n",
      "Epoch 4/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 5/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 6/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1792     \n",
      "Epoch 7/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 8/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1636     \n",
      "Epoch 9/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1733     \n",
      "Epoch 10/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1772     \n",
      "Epoch 11/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1792     \n",
      "Epoch 12/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1792     \n",
      "Epoch 13/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 14/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1500     \n",
      "Epoch 15/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1811     \n",
      "Epoch 16/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 17/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1792     \n",
      "Epoch 18/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1792     \n",
      "Epoch 19/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 20/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1694     \n",
      "Epoch 21/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1792     \n",
      "Epoch 22/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1792     \n",
      "Epoch 23/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1792     \n",
      "Epoch 24/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1792     \n",
      "Epoch 25/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1792     \n",
      "Epoch 1/25\n",
      "1027/1027 [==============================] - 4s - loss: 0.1566 - cosine_proximity: -0.1607     \n",
      "Epoch 2/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1704     \n",
      "Epoch 3/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1685     \n",
      "Epoch 4/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1529     \n",
      "Epoch 5/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1704     \n",
      "Epoch 6/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1723     \n",
      "Epoch 7/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1704     \n",
      "Epoch 8/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1704     - ETA: 0s - loss: 0.1563 - cosine_proximity: -0.1\n",
      "Epoch 9/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1704     \n",
      "Epoch 10/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1392     \n",
      "Epoch 11/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1587     \n",
      "Epoch 12/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1704     \n",
      "Epoch 13/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1665     \n",
      "Epoch 14/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1685     \n",
      "Epoch 15/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1665     \n",
      "Epoch 16/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1470     \n",
      "Epoch 17/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1704     \n",
      "Epoch 18/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1646     \n",
      "Epoch 19/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1685     \n",
      "Epoch 20/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1704     \n",
      "Epoch 21/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1704     \n",
      "Epoch 22/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1704     \n",
      "Epoch 23/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1626     \n",
      "Epoch 24/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1704     \n",
      "Epoch 25/25\n",
      "1027/1027 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1704     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 4s - loss: 0.1581 - cosine_proximity: -0.1333     - ETA: 9s - loss: 0.1544 - cosine_proximity: -0.044 - ETA: 8s - loss: 0.1578 - \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1576 - cosine_proximity: -0.1420     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1579 - cosine_proximity: -0.1693     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1579 - cosine_proximity: -0.1420     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1578 - cosine_proximity: -0.1323     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1577 - cosine_proximity: -0.1654     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1579 - cosine_proximity: -0.1595     - ETA: 0s - loss: 0.1579 - cosine_proximity: -0.1\n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1581 - cosine_proximity: -0.1420     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1578 - cosine_proximity: -0.1693     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1575 - cosine_proximity: -0.1693     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1579 - cosine_proximity: -0.0973     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1580 - cosine_proximity: -0.1070     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1578 - cosine_proximity: -0.1459     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1574 - cosine_proximity: -0.1751     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1575 - cosine_proximity: -0.1712     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1576 - cosine_proximity: -0.1420     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1581 - cosine_proximity: -0.0992     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1578 - cosine_proximity: -0.1712     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1576 - cosine_proximity: -0.1693     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1577 - cosine_proximity: -0.1634     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1578 - cosine_proximity: -0.1693     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1580 - cosine_proximity: -0.1693     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1577 - cosine_proximity: -0.1693     \n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1577 - cosine_proximity: -0.1615     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1575 - cosine_proximity: -0.1615     - ETA: 0s - loss: 0.1557 - cosine_proximity:\n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 4s - loss: 0.1568 - cosine_proximity: -0.1644     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1663     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1352     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1547     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1119     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1722     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1741     \n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1722     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1508     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1605     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1605     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1722     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1352     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1722     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1565 - cosine_proximity: -0.1722     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1722     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1586     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1683     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1741     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1722     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1488     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1722     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1741     \n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1722     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1566 - cosine_proximity: -0.1722     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 4s - loss: 0.1556 - cosine_proximity: -0.1566     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - ETA: 0s - loss: 0.1554 - cosine_proximity: -0.158 - 2s - loss: 0.1552 - cosine_proximity: -0.1576     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1206     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1732     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1245     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1732     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1732     - ETA: 1s - loss: 0.1438 - cosine\n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1732     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1712     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1549 - cosine_proximity: -0.1732     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1576     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1128     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1551 - cosine_proximity: -0.1693     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1551 - cosine_proximity: -0.1732     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1732     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1732     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1732     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1693     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1732     - ETA: 1s - loss: 0.1503 - cosine_pro\n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1732     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 3s - loss: 0.1553 - cosine_proximity: -0.1732     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1732     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1712     \n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1552 - cosine_proximity: -0.1693     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1654     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 5s - loss: 0.1565 - cosine_proximity: -0.1352     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1809     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1790     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1790     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1790     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1790     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1790     \n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1790     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1770     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1770     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1790     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1790     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1790     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1790     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1790     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1790     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1790     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1790     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1790     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1790     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1558 - cosine_proximity: -0.1790     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1809     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1790     \n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1790     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1790     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 5s - loss: 0.1519 - cosine_proximity: -0.0798     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1522 - cosine_proximity: -0.1323     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1518 - cosine_proximity: -0.1595     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1521 - cosine_proximity: -0.1304     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1521 - cosine_proximity: -0.1654     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1525 - cosine_proximity: -0.1654     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1521 - cosine_proximity: -0.1693     \n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1523 - cosine_proximity: -0.1654     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1523 - cosine_proximity: -0.1634     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1521 - cosine_proximity: -0.1634     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1520 - cosine_proximity: -0.1654     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1519 - cosine_proximity: -0.1654     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1521 - cosine_proximity: -0.1304     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1523 - cosine_proximity: -0.1654     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1520 - cosine_proximity: -0.1128     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1519 - cosine_proximity: -0.1342     ETA: 1s - loss: 0.1460 -\n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1519 - cosine_proximity: -0.1654     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1523 - cosine_proximity: -0.1518     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1522 - cosine_proximity: -0.1323     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1520 - cosine_proximity: -0.1654     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1520 - cosine_proximity: -0.1654     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1523 - cosine_proximity: -0.1537     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - ETA: 0s - loss: 0.1523 - cosine_proximity: -0.1680- ETA: 0s - loss: 0.1522 - cosine_proximity: -0.1 - 2s - loss: 0.1520 - cosine_proximity: -0.1654     \n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1521 - cosine_proximity: -0.1556     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1518 - cosine_proximity: -0.1654     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 5s - loss: 0.1547 - cosine_proximity: -0.1099     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1541 - cosine_proximity: -0.1488     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1546 - cosine_proximity: -0.1060     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1819     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1544 - cosine_proximity: -0.1683     - ETA: 1s - loss: 0.1519 - cos\n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1839     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1544 - cosine_proximity: -0.1235     \n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1800     - ETA: 1s - loss: 0.1495 - cosine_pr\n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1625     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1546 - cosine_proximity: -0.1819     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1544 - cosine_proximity: -0.1800     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1819     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1544 - cosine_proximity: -0.1819     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1819     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1819     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1540 - cosine_proximity: -0.1819     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1541 - cosine_proximity: -0.1819     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1819     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1819     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1819     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1541 - cosine_proximity: -0.1819     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1819     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1543 - cosine_proximity: -0.1819     \n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1819     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1542 - cosine_proximity: -0.1819     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 5s - loss: 0.1536 - cosine_proximity: -0.1488     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1536 - cosine_proximity: -0.1819     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1508     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1780     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1537 - cosine_proximity: -0.1819     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1535 - cosine_proximity: -0.1819     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1819     \n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1216     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1216     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1537 - cosine_proximity: -0.1819     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1531 - cosine_proximity: -0.1800     - ETA: 1s - loss: 0.1348 - \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1535 - cosine_proximity: -0.1741     \n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1536 - cosine_proximity: -0.1819     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1819     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1532 - cosine_proximity: -0.1800     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1537 - cosine_proximity: -0.1819     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1839     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1819     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1644     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1761     - ETA: 1s - loss: 0.1556 - cosine_proximity: -0 - ETA: 1s - loss: 0.1548 - cosin\n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1819     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1535 - cosine_proximity: -0.1819     \n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1819     - ETA: 1s - loss: 0.1537 - cosine_p\n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1534 - cosine_proximity: -0.1819     \n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1533 - cosine_proximity: -0.1819     \n",
      "Epoch 1/25\n",
      "1028/1028 [==============================] - 5s - loss: 0.1561 - cosine_proximity: -0.1722     \n",
      "Epoch 2/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1926     \n",
      "Epoch 3/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1965     \n",
      "Epoch 4/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1557 - cosine_proximity: -0.1537     \n",
      "Epoch 5/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1965     \n",
      "Epoch 6/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1965     \n",
      "Epoch 7/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1946     \n",
      "Epoch 8/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1946     \n",
      "Epoch 9/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.2004     \n",
      "Epoch 10/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1965     \n",
      "Epoch 11/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1563 - cosine_proximity: -0.1887     \n",
      "Epoch 12/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1965     - ETA: 1s - loss: 0.1509 -\n",
      "Epoch 13/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1926     \n",
      "Epoch 14/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1965     \n",
      "Epoch 15/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1562 - cosine_proximity: -0.1965     \n",
      "Epoch 16/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1556 - cosine_proximity: -0.1304     \n",
      "Epoch 17/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1965     \n",
      "Epoch 18/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1559 - cosine_proximity: -0.1946     \n",
      "Epoch 19/25\n",
      "1028/1028 [==============================] - ETA: 0s - loss: 0.1562 - cosine_proximity: -0.193 - 2s - loss: 0.1560 - cosine_proximity: -0.1965     \n",
      "Epoch 20/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1564 - cosine_proximity: -0.1965     \n",
      "Epoch 21/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1965     \n",
      "Epoch 22/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1560 - cosine_proximity: -0.1965     - ETA: 0s - loss: 0.1557 - cosine_proximity: -\n",
      "Epoch 23/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1965     \n",
      "Epoch 24/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1965     - ETA: 0s - loss: 0.1547 - cosine_proxi\n",
      "Epoch 25/25\n",
      "1028/1028 [==============================] - 2s - loss: 0.1561 - cosine_proximity: -0.1946     \n",
      "Epoch 1/25\n",
      "1142/1142 [==============================] - 6s - loss: 0.1556 - cosine_proximity: -0.1462     \n",
      "Epoch 2/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1191     \n",
      "Epoch 3/25\n",
      "1142/1142 [==============================] - 3s - loss: 0.1553 - cosine_proximity: -0.1769     - ETA: 1s - loss: 0.1615 - cosine\n",
      "Epoch 4/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 5/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 6/25\n",
      "1142/1142 [==============================] - ETA: 0s - loss: 0.1540 - cosine_proximity: -0.164 - 2s - loss: 0.1555 - cosine_proximity: -0.1664     \n",
      "Epoch 7/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1524     \n",
      "Epoch 8/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 9/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1716     \n",
      "Epoch 10/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1769     \n",
      "Epoch 11/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1769     \n",
      "Epoch 12/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1646     - ETA: 0s - loss: 0.1529 - cosine_proximity: \n",
      "Epoch 13/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 14/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1769     \n",
      "Epoch 15/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 16/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1549 - cosine_proximity: -0.1734     \n",
      "Epoch 17/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 18/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1551 - cosine_proximity: -0.1699     \n",
      "Epoch 19/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1555 - cosine_proximity: -0.1769     \n",
      "Epoch 20/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1551 - cosine_proximity: -0.1769     \n",
      "Epoch 21/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1769     \n",
      "Epoch 22/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1751     \n",
      "Epoch 23/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1769     \n",
      "Epoch 24/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1553 - cosine_proximity: -0.1769     \n",
      "Epoch 25/25\n",
      "1142/1142 [==============================] - 2s - loss: 0.1554 - cosine_proximity: -0.1734     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f67fe8f9fd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweeked_lstm = TweekedLSTM(fin_word2vec_model)\n",
    "tweeked_res = tweeked_lstm.cross_validate(train_texts, train_sentiments)\n",
    "tweeked_lstm.fit(train_texts, train_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweek_error_details, tweek_error_dist = error_analysis(test_texts, test_sentiments,\n",
    "                                                            test_companies, tweeked_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [{'Company': 'bp',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'bp reports biggest ever annual loss',\n",
       "   'True value': -0.98999999999999999,\n",
       "   'index': 415},\n",
       "  {'Company': 'glencore',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'glencore shares in record crash as profit fears grow',\n",
       "   'True value': -0.97099999999999997,\n",
       "   'index': 142},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'oil giant bp reports loss of $4.4 billion in 4th quarter of 2014',\n",
       "   'True value': -0.96299999999999997,\n",
       "   'index': 83},\n",
       "  {'Company': 'aviva plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'aviva posts forecast-beating 2015 operating profit of $3.8 bln',\n",
       "   'True value': 0.94599999999999995,\n",
       "   'index': 257},\n",
       "  {'Company': 'persimmon',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'update: persimmon profit up strongly, outlook positive',\n",
       "   'True value': 0.93700000000000006,\n",
       "   'index': 442},\n",
       "  {'Company': 'morrisons',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'companiesmorrison evicted from ftse 100 as worldpay joins',\n",
       "   'True value': -0.85199999999999998,\n",
       "   'index': 31},\n",
       "  {'Company': 'volkswagen',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'us subpoenas volkswagen under bank fraud law in emissions probe: source',\n",
       "   'True value': -0.83799999999999997,\n",
       "   'index': 149},\n",
       "  {'Company': 'hsbc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'hsbc reaches $470 million accord over foreclosure abuses',\n",
       "   'True value': -0.83499999999999996,\n",
       "   'index': 320},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'barclays faces another heavy forex fine',\n",
       "   'True value': -0.83399999999999996,\n",
       "   'index': 4},\n",
       "  {'Company': 'old mutual',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'old mutual shares soar on break-up report',\n",
       "   'True value': 0.91100000000000003,\n",
       "   'index': 327},\n",
       "  {'Company': 'weir group',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'slump in weir leads ftse down from record high',\n",
       "   'True value': -0.82699999999999996,\n",
       "   'index': 284},\n",
       "  {'Company': 'm&g investments',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'aviva, m&g suspend property funds as investors panic',\n",
       "   'True value': -0.80700000000000005,\n",
       "   'index': 80},\n",
       "  {'Company': 'volkswagen',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'germany lags european share rally as volkswagen scandal widens',\n",
       "   'True value': -0.80300000000000005,\n",
       "   'index': 249},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'copper falls with mining stocks as barclays warns of pullback',\n",
       "   'True value': -0.80000000000000004,\n",
       "   'index': 487},\n",
       "  {'Company': 'fresnillo',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'companiesfresnillo shares jump 8% as silver price breaks $21',\n",
       "   'True value': 0.874,\n",
       "   'index': 394},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'foreign exchange probes push rbs to â£446m loss',\n",
       "   'True value': -0.79100000000000004,\n",
       "   'index': 479},\n",
       "  {'Company': 'wolseley',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'marketswolseley shares wilt 8.8% after full year results',\n",
       "   'True value': -0.78700000000000003,\n",
       "   'index': 158},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'warning as rbs slumps to â£446m loss',\n",
       "   'True value': -0.78200000000000003,\n",
       "   'index': 303},\n",
       "  {'Company': 'glencore',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'glencore slumps 30 percent as debt fears grow',\n",
       "   'True value': -0.78100000000000003,\n",
       "   'index': 280},\n",
       "  {'Company': 'tesco plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'tesco share price tumbles after negative broker note',\n",
       "   'True value': -0.77900000000000003,\n",
       "   'index': 445},\n",
       "  {'Company': 'london stock exchange group',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'news feedftse 100 movers: lse surges as ice says mulling offer; ashtead and barclays tank',\n",
       "   'True value': 0.84299999999999997,\n",
       "   'index': 386},\n",
       "  {'Company': 'shire',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'homebuilders, shire gains spur ftse 100 to rally beyond record',\n",
       "   'True value': 0.83699999999999997,\n",
       "   'index': 11},\n",
       "  {'Company': 'bhp billiton',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'update 2-bhp billiton profit dives to 10-year low on commodities rout',\n",
       "   'True value': -0.754,\n",
       "   'index': 381},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'update 2-three former barclays traders found guilty in libor rigging trial',\n",
       "   'True value': -0.74299999999999999,\n",
       "   'index': 49},\n",
       "  {'Company': 'morrisons',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'market report: new morrisons chief executive buys another â£500000 worth of ...',\n",
       "   'True value': 0.82199999999999995,\n",
       "   'index': 41},\n",
       "  {'Company': 'wilshire bancorp',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'bbcn bancorp to buy wilshire bancorp in $1 bln deal',\n",
       "   'True value': 0.81799999999999995,\n",
       "   'index': 363},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'bp to slash 4000 jobs globally as oil prices drop',\n",
       "   'True value': -0.73599999999999999,\n",
       "   'index': 178},\n",
       "  {'Company': 'intertek group',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': \"intertek swings to â£347 mln loss on oil's slump\",\n",
       "   'True value': -0.73399999999999999,\n",
       "   'index': 293},\n",
       "  {'Company': 'morrisons',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'industry newsmorrisons unveils which stores will close as 900 jobs face the axe',\n",
       "   'True value': -0.73099999999999998,\n",
       "   'index': 456},\n",
       "  {'Company': 'standard life',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'property stocks slump after standard life freezes u.k. fund',\n",
       "   'True value': -0.71799999999999997,\n",
       "   'index': 136},\n",
       "  {'Company': 'glencore',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'companiesglencore shares hit 3-month high after refinancing key credit line',\n",
       "   'True value': 0.79500000000000004,\n",
       "   'index': 25},\n",
       "  {'Company': 'lloyds banking group plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'lloyds banking group reports 7% dip in annual profits',\n",
       "   'True value': -0.69599999999999995,\n",
       "   'index': 432},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'barclays shares dive as plan for ring-fencing costs extra â£1 billion',\n",
       "   'True value': -0.69299999999999995,\n",
       "   'index': 387},\n",
       "  {'Company': 'antofagasta',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'industry newsstrong end to a tough year for antofagasta',\n",
       "   'True value': 0.77100000000000002,\n",
       "   'index': 59},\n",
       "  {'Company': 'j sainsbury plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'why i would put j sainsbury plc in my trolley before wm morrison supermarkets ...',\n",
       "   'True value': 0.76400000000000001,\n",
       "   'index': 323},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'judge: 22 bp fraud charges against lawyer duplicate others',\n",
       "   'True value': -0.67800000000000005,\n",
       "   'index': 173},\n",
       "  {'Company': 'acerta',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'royal mail fined â£40m after french probe',\n",
       "   'True value': -0.67100000000000004,\n",
       "   'index': 79},\n",
       "  {'Company': 'sunray',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'sunray cuts jobs as new hampshire reaches cap on solar incentive',\n",
       "   'True value': -0.67000000000000004,\n",
       "   'index': 338},\n",
       "  {'Company': 'barclays',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': \"industry newsbarclays to 'cut investment bank staff by another 20%'\",\n",
       "   'True value': -0.66700000000000004,\n",
       "   'index': 261},\n",
       "  {'Company': 'direct line insurance group plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'direct line raises 2015 guidance on combined ratio improvement',\n",
       "   'True value': 0.746,\n",
       "   'index': 166},\n",
       "  {'Company': 'london stock exchange group',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'london stock exchange boosts ftse on new bid hopes',\n",
       "   'True value': 0.74299999999999999,\n",
       "   'index': 312},\n",
       "  {'Company': 'land securities',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'land securities share price: group reveals strong full-year results',\n",
       "   'True value': 0.73999999999999999,\n",
       "   'index': 314},\n",
       "  {'Company': 'bp',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': \"how big was bp's $5.8 billion loss?\",\n",
       "   'True value': -0.65100000000000002,\n",
       "   'index': 65},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'rbs shares drop after surprise pension charge cuts capital',\n",
       "   'True value': -0.65100000000000002,\n",
       "   'index': 439},\n",
       "  {'Company': 'bhp billiton',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'miners meltdown as bhp to rio tinto sink in commodities rout',\n",
       "   'True value': -0.64600000000000002,\n",
       "   'index': 379},\n",
       "  {'Company': 'old mutual',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'rpt-old mutual q1 gross sales beat forecasts, up 18 pct',\n",
       "   'True value': 0.72399999999999998,\n",
       "   'index': 91},\n",
       "  {'Company': 'royal bank of scotland group plc',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'rbs turns away savers in latest glitch',\n",
       "   'True value': -0.64200000000000002,\n",
       "   'index': 191}],\n",
       " 2: [{'Company': 'astrazeneca',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'why astrazeneca plc & dixons carphone plc are red-hot growth stars!',\n",
       "   'True value': 0.93400000000000005,\n",
       "   'index': 245},\n",
       "  {'Company': 'dixons carphone',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': 'why astrazeneca plc & dixons carphone plc are red-hot growth stars!',\n",
       "   'True value': 0.76600000000000001,\n",
       "   'index': 437},\n",
       "  {'Company': 'prudential',\n",
       "   'Pred value': array([ 0.0399968], dtype=float32),\n",
       "   'Sentence': \"uk's ftse has worst day so far in 2015 as bg and prudential fall\",\n",
       "   'True value': -0.65100000000000002,\n",
       "   'index': 2}]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweek_error_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 47, 2: 3}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweek_error_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric 1 0.03579421134515637\n",
      "Metric 2 0.123826853165601\n",
      "Metric 3 0.4163761234895398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    }
   ],
   "source": [
    "pred_values = eval_format(test_texts, tweeked_lstm.predict(test_texts))\n",
    "print('Metric 1 {}'.format(eval_func(true_values, pred_values, metric=metric1)))\n",
    "print('Metric 2 {}'.format(eval_func(true_values, pred_values, metric=metric2)))\n",
    "print('Metric 3 {}'.format(eval_func(true_values, pred_values, metric=metric3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweeked lstm cross val score 7.7931559069293765\n",
      "Early lstm cross val score 7.820167801195983\n"
     ]
    }
   ],
   "source": [
    "# Print both LSTM's cross validation results\n",
    "avg_tweek_percentage = (sum(tweeked_res) / len(tweeked_res)) * 100\n",
    "print('Tweeked lstm cross val score {}'.format(avg_tweek_percentage))\n",
    "\n",
    "avg_early_percentage = (sum(early_res) / len(early_res)) * 100\n",
    "print('Early lstm cross val score {}'.format(avg_early_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
